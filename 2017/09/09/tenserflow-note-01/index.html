<!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="1EB8XoOl0C"><meta name="google-site-verification" content="K7thEgdLm0UfRWJ5MGdF7sCcjClSzAlxFLPv2Oz5CGM"><title> tenserflow笔记-01 · yxleung</title><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="tenserflow笔记-01 - yxleung"><meta name="keywords"><meta name="author" content="yxleung"><link rel="short icon" href="/"><link rel="stylesheet" href="/css/bubuzou.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yxleung.github.io/atom.xml" title="yxleung"></head><body><header><div class="header row"> <a href="/" class="logo-link"><img></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" data-hover="博文" class="nav-list-link">博文</a></li><li class="nav-list-item"><a href="/categories/technology/" target="_self" data-hover="技术" class="nav-list-link">技术</a></li><li class="nav-list-item"><a href="/archives/" target="_self" data-hover="归档" class="nav-list-link">归档</a></li><li class="nav-list-item"><a href="/about/" target="_self" data-hover="关于" class="nav-list-link">关于</a></li></ul><div class="search"><a id="search_btn" href="#search"></a></div></div></header><div class="row scroll-con"><section class="container"><!-- for archive page--><div id="postAr" class="post"><article class="post-block"><h1 class="post-title">tenserflow笔记-01</h1><div class="post-info">2017-09-09<p class="visit"><i data-hk-page="current">-</i><span>次访问</span></p></div><div class="post-content"><h2 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h2><p>让计算机自己去解决问题，神经网络就是一个函数，从训练数据集中求出多元方程组的解，之后使用这个函数，求出输入对应的预期输出。</p>
<p>简单来说，神经网络就是能够从训练数据集学习的函数。</p>
<h2 id="使用神经网络过程："><a href="#使用神经网络过程：" class="headerlink" title="使用神经网络过程："></a>使用神经网络过程：</h2><ol>
<li>首先要构建一个解决某一个问题的神经网络。（创建函数）</li>
<li>使用大量训练数据集，训练网络。（求解多元 方程组）</li>
<li>最终得到一个神经网络（函数），输入数据，输出期望信息。</li>
</ol>
<h2 id="简单原理"><a href="#简单原理" class="headerlink" title="简单原理"></a>简单原理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">公式：W1x1 + W2x2 &gt; b</div></pre></td></tr></table></figure>
<ol>
<li>W1、W2：weigh权重，用于改变分类直线的角度。神经元角度（W1、W2代表了输入与神经元连接的强度）</li>
<li>b:bias 偏置，用于改变分类直线的位置。神经元角度（b表示一个神经元是否可被输入激活的阈值）</li>
</ol>
<h6 id="所以可以重复使用这个公式，分类任何可以被一条直线分类的数据集。"><a href="#所以可以重复使用这个公式，分类任何可以被一条直线分类的数据集。" class="headerlink" title="所以可以重复使用这个公式，分类任何可以被一条直线分类的数据集。"></a>所以可以重复使用这个公式，分类任何可以被一条直线分类的数据集。</h6><h4 id="问题：怎么找到W1、W2、b的值，即所谓参数值。"><a href="#问题：怎么找到W1、W2、b的值，即所谓参数值。" class="headerlink" title="问题：怎么找到W1、W2、b的值，即所谓参数值。"></a>问题：怎么找到W1、W2、b的值，即所谓参数值。</h4><blockquote>
<p>训练神经元，反向传播和梯度下降算法<br>训练过程中的计算机会尝试一点点增大或减小每个参数，看其能如何减少相比于训练 数据集的误差，以望能找到最优的参数组合。</p>
</blockquote>
<h3 id="找到了W1，W2，b，即可以使用神经元做分类。"><a href="#找到了W1，W2，b，即可以使用神经元做分类。" class="headerlink" title="找到了W1，W2，b，即可以使用神经元做分类。"></a>找到了W1，W2，b，即可以使用神经元做分类。</h3><p>一个神经元所能做的唯一事情：使用权重和偏置检查输入值，将一个数据点分到两类中的一类。</p>
<p>带有两个输入时，一个神经元可以使用一条直线将一个数据点在一个二维空间中分成两类。如果有三个输入，一个神经元可以使用一个平面将一个三维空间分成两部分，以此类推。这就是所谓的「使用一个超平面分割 n 维空间（dividing n-dimensional space with a hyperplane）」。</p>
<h4 id="一个神经元可以将任何数据点分为两类"><a href="#一个神经元可以将任何数据点分为两类" class="headerlink" title="一个神经元可以将任何数据点分为两类"></a>一个神经元可以将任何数据点分为两类</h4><h2 id="隐含层"><a href="#隐含层" class="headerlink" title="隐含层"></a>隐含层</h2><p>使用隐含层，将输入变换为特征空间，把非线性的问题，使其线性分类。</p>
</div></article></div><div class="right-container"><div class="widget"><div class="category"><h4>分类归档</h4><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/technology/">technology</a><span class="category-list-count">4</span></li></ul></div></div><div class="widget"><div class="tagcloud"><h4>标签云</h4><a href="/tags/gnuroot-debian/" style="font-size: 10px;">gnuroot debian</a> <a href="/tags/shadowsocks-R2D-交叉编译-小米路由器/" style="font-size: 10px;">shadowsocks R2D 交叉编译 小米路由器</a> <a href="/tags/tenserflow/" style="font-size: 10px;">tenserflow</a> <a href="/tags/特征工程-信息熵-香农熵-entropy/" style="font-size: 10px;">特征工程 信息熵 香农熵 entropy</a></div></div><div class="widget"><div class="recent"><h4>最近文章</h4><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/01/02/交叉编译小米路由器2（R2D）shadowsocks/">交叉编译小米路由器2（R2D）shadowsocks</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/11/entropy-note-01/">entropy-note-01</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/09/GNUroot-android-ssh/">GNUroot debian android ssh设置</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/09/tenserflow-note-01/">tenserflow笔记-01</a></li></ul></div></div><div class="widget"><div id="arAnchorBar"></div></div></div></section></div><div class="right-menu"></div><div class="modal search-modal"><div class="input-field"><input type="text" id="search_input"><label for="search-input">搜索</label></div><div id="search_result" class="search-result"></div></div><div class="blog-overlay"></div><footer class="row"><div class="footer-con"><div class="paginator"><a href="/2017/09/09/GNUroot-android-ssh/" class="prev">PREV</a></div><div class="copyright"><p>© 2017 - 2018 <a href="http://yxleung.github.io" target="_blank">yxleung</a></p><p> </p></div><div class="totop"><i></i></div></div></footer><script async src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="http://apps.bdimg.com/libs/jquery/1.8.2/jquery.min.js"></script><script src="https://cdn1.lncld.net/static/js/av-mini-0.6.10.js"></script><script src="/scripts/hit-kounter-lc-0.2.0.js"></script><script src="/scripts/arAnchor.js"></script><script src="/scripts/main.js"></script></body></html>